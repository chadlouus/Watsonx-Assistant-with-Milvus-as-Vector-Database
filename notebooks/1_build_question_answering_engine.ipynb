{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ruslanmv/Watsonx-Assistant-with-Milvus-as-Vector-Database/blob/master/notebooks/1_build_question_answering_engine.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e410b6c4",
      "metadata": {
        "id": "e410b6c4"
      },
      "source": [
        "# Build a Question Answering Engine in Minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bed6f24",
      "metadata": {
        "id": "9bed6f24"
      },
      "source": [
        "This notebook illustrates how to build a question answering engine from scratch using [Milvus](https://milvus.io/) and [Towhee](https://towhee.io/). Milvus is the most advanced open-source vector database built for AI applications and supports nearest neighbor embedding search across tens of millions of entries, and Towhee is a framework that provides ETL for unstructured data using SoTA machine learning models.\n",
        "\n",
        "We will go through question answering procedures and evaluate performance. Moreover, we managed to make the core functionality as simple as almost 10 lines of code with Towhee, so that you can start hacking your own question answering engine."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4883e577",
      "metadata": {
        "id": "4883e577"
      },
      "source": [
        "## Preparations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49110b91",
      "metadata": {
        "id": "49110b91"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0117995a",
      "metadata": {
        "id": "0117995a"
      },
      "source": [
        "First we need to install dependencies such as towhee, towhee.models and gradio."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c9ba3850",
      "metadata": {
        "id": "c9ba3850",
        "outputId": "64286339-3996-4317-c6c6-6582e74a8eec"
      },
      "outputs": [],
      "source": [
        "! python -m pip install -q towhee towhee.models gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a90db0c5",
      "metadata": {
        "id": "a90db0c5"
      },
      "source": [
        "### Prepare the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1eceb58",
      "metadata": {
        "id": "d1eceb58"
      },
      "source": [
        "There is a subset of the  [InsuranceQA Corpus](https://github.com/shuzi/insuranceQA)  (1000 pairs of questions and answers) used in this demo, everyone can download on [Github](https://github.com/towhee-io/examples/releases/download/data/question_answer.csv)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "d1436a9c",
      "metadata": {
        "id": "d1436a9c",
        "outputId": "17c7f2ff-79fb-4b04-bcd5-79a048f8e4a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  595k  100  595k    0     0   601k      0 --:--:-- --:--:-- --:--:-- 1048k\n"
          ]
        }
      ],
      "source": [
        "! curl -L https://github.com/towhee-io/examples/releases/download/data/question_answer.csv -O"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4abdc0a",
      "metadata": {
        "id": "c4abdc0a"
      },
      "source": [
        "**question_answer.csv**: a file containing question and the answer.\n",
        "\n",
        "Let's take a quick look:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d652efea",
      "metadata": {
        "id": "d652efea",
        "outputId": "ee6fdd36-8205-40d0-b7b2-701c09f153f7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_700/3825667354.py:1: DeprecationWarning: \n",
            "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
            "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
            "but was not found to be installed on your system.\n",
            "If this would cause problems for you,\n",
            "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
            "        \n",
            "  import pandas as pd\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>question</th>\n",
              "      <th>answer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Is  Disability  Insurance  Required  By  Law?</td>\n",
              "      <td>Not generally. There are five states that requ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Can  Creditors  Take  Life  Insurance  After  ...</td>\n",
              "      <td>If the person who passed away was the one with...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Does  Travelers  Insurance  Have  Renters  Ins...</td>\n",
              "      <td>One of the insurance carriers I represent is T...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Can  I  Drive  A  New  Car  Home  Without  Ins...</td>\n",
              "      <td>Most auto dealers will not let you drive the c...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Is  The  Cash  Surrender  Value  Of  Life  Ins...</td>\n",
              "      <td>Cash surrender value comes only with Whole Lif...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id                                           question  \\\n",
              "0   0      Is  Disability  Insurance  Required  By  Law?   \n",
              "1   1  Can  Creditors  Take  Life  Insurance  After  ...   \n",
              "2   2  Does  Travelers  Insurance  Have  Renters  Ins...   \n",
              "3   3  Can  I  Drive  A  New  Car  Home  Without  Ins...   \n",
              "4   4  Is  The  Cash  Surrender  Value  Of  Life  Ins...   \n",
              "\n",
              "                                              answer  \n",
              "0  Not generally. There are five states that requ...  \n",
              "1  If the person who passed away was the one with...  \n",
              "2  One of the insurance carriers I represent is T...  \n",
              "3  Most auto dealers will not let you drive the c...  \n",
              "4  Cash surrender value comes only with Whole Lif...  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('question_answer.csv')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "309bfb43",
      "metadata": {
        "id": "309bfb43"
      },
      "source": [
        "To use the dataset to get answers, let's first define the dictionary:\n",
        "\n",
        "- `id_answer`: a dictionary of id and corresponding answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4d98b309",
      "metadata": {
        "id": "4d98b309"
      },
      "outputs": [],
      "source": [
        "id_answer = df.set_index('id')['answer'].to_dict()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c5a0858",
      "metadata": {
        "id": "1c5a0858"
      },
      "source": [
        "### Create Milvus Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efb06a01",
      "metadata": {
        "id": "efb06a01"
      },
      "source": [
        "Before getting started, please make sure that you have started a [Milvus service](https://milvus.io/docs/install_standalone-docker.md). This notebook uses [milvus 2.2.10](https://milvus.io/docs/v2.2.x/install_standalone-docker.md) and [pymilvus 2.2.11](https://milvus.io/docs/release_notes.md#2210)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8048bf6c",
      "metadata": {
        "id": "8048bf6c"
      },
      "outputs": [],
      "source": [
        "! python -m pip install -q pymilvus==2.2.11"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87ba2b23",
      "metadata": {
        "id": "87ba2b23"
      },
      "source": [
        "Next to define the function `create_milvus_collection` to create collection in Milvus that uses the [L2 distance metric](https://milvus.io/docs/metric.md#Euclidean-distance-L2) and an [IVF_FLAT index](https://milvus.io/docs/index.md#IVF_FLAT)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "344421fe",
      "metadata": {},
      "source": [
        "### Setup Remote Server\n",
        "Here we should define the variable `REMOTE_SERVER` just created [here](https://github.com/ruslanmv/Watsonx-Assistant-with-Milvus-as-Vector-Database/blob/master/README.md)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f4105b31",
      "metadata": {},
      "outputs": [],
      "source": [
        "REMOTE_SERVER='50.17.92.90'\n",
        "LOCAL_SERVER='127.0.0.1'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "68e15f49",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pymilvus import connections, FieldSchema, CollectionSchema, DataType, Collection, utility\n",
        "connections.connect(host=REMOTE_SERVER, port='19530')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "22c19982",
      "metadata": {
        "id": "22c19982"
      },
      "outputs": [],
      "source": [
        "def create_milvus_collection(collection_name, dim):\n",
        "    if utility.has_collection(collection_name):\n",
        "        utility.drop_collection(collection_name)\n",
        "\n",
        "    fields = [\n",
        "    FieldSchema(name='id', dtype=DataType.VARCHAR, descrition='ids', max_length=500, is_primary=True, auto_id=False),\n",
        "    FieldSchema(name='embedding', dtype=DataType.FLOAT_VECTOR, descrition='embedding vectors', dim=dim)\n",
        "    ]\n",
        "    schema = CollectionSchema(fields=fields, description='reverse image search')\n",
        "    collection = Collection(name=collection_name, schema=schema)\n",
        "\n",
        "    # create IVF_FLAT index for collection.\n",
        "    index_params = {\n",
        "        'metric_type':'L2',\n",
        "        'index_type':\"IVF_FLAT\",\n",
        "        'params':{\"nlist\":2048}\n",
        "    }\n",
        "    collection.create_index(field_name=\"embedding\", index_params=index_params)\n",
        "    return collection\n",
        "\n",
        "collection = create_milvus_collection('question_answer', 768)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9724ba28",
      "metadata": {
        "id": "9724ba28"
      },
      "source": [
        "## Question Answering Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01e1ac7e",
      "metadata": {
        "id": "01e1ac7e"
      },
      "source": [
        "In this section, we will show how to build our question answering engine using Milvus and Towhee. The basic idea behind question answering is to use Towhee to generate embedding from the question dataset and compare the input question with the embedding stored in Milvus.\n",
        "\n",
        "[Towhee](https://towhee.io/) is a machine learning framework that allows the creation of data processing pipelines, and it also provides predefined operators for implementing insert and query operations in Milvus.\n",
        "\n",
        "<img src=\"./workflow.png\" width = \"60%\" height = \"60%\" align=center />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c0188bf",
      "metadata": {
        "id": "4c0188bf"
      },
      "source": [
        "### Load question embedding into Milvus"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a654fdc",
      "metadata": {
        "id": "0a654fdc"
      },
      "source": [
        "We first generate embedding from question text with [dpr](https://towhee.io/text-embedding/dpr) operator and insert the embedding into Milvus. Towhee provides a [method-chaining style API](https://towhee.readthedocs.io/en/main/index.html) so that users can assemble a data processing pipeline with operators."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "e6fa8061",
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "13b7beea",
      "metadata": {
        "id": "13b7beea",
        "outputId": "5152283b-025b-43f3-b0f3-215a174c285a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 26.9 s, sys: 2.03 s, total: 28.9 s\n",
            "Wall time: 3min 29s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "from towhee import pipe, ops\n",
        "import numpy as np\n",
        "from towhee.datacollection import DataCollection\n",
        "from IPython.display import clear_output\n",
        "insert_pipe = (\n",
        "    pipe.input('id', 'question', 'answer')\n",
        "        .map('question', 'vec', ops.text_embedding.dpr(model_name='facebook/dpr-ctx_encoder-single-nq-base'))\n",
        "        .map('vec', 'vec', lambda x: x / np.linalg.norm(x, axis=0))\n",
        "        .map(('id', 'vec'), 'insert_status', ops.ann_insert.milvus_client(host=REMOTE_SERVER, port='19530', collection_name='question_answer'))\n",
        "        .output()\n",
        ")\n",
        "\n",
        "import csv\n",
        "with open('question_answer.csv', encoding='utf-8') as f:\n",
        "    reader = csv.reader(f)\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "        insert_pipe(*row)\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "1adbb2e1",
      "metadata": {
        "id": "1adbb2e1",
        "outputId": "b6dea15e-3ae8-4263-d6d3-7c1822b3ba19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total number of inserted data is 0.\n"
          ]
        }
      ],
      "source": [
        "print('Total number of inserted data is {}.'.format(collection.num_entities))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deb269f4",
      "metadata": {
        "id": "deb269f4"
      },
      "source": [
        "#### Explanation of Data Processing Pipeline\n",
        "\n",
        "Here is detailed explanation for each line of the code:\n",
        "\n",
        "`pipe.input('id', 'question', 'answer')`: Get three inputs, namely question's id, quesion's text and question's answer;\n",
        "\n",
        "`map('question', 'vec', ops.text_embedding.dpr(model_name='facebook/dpr-ctx_encoder-single-nq-base'))`: Use the `acebook/dpr-ctx_encoder-single-nq-base` model to generate the question embedding vector with the [dpr operator](https://towhee.io/text-embedding/dpr) in towhee hub;\n",
        "\n",
        "`map('vec', 'vec', lambda x: x / np.linalg.norm(x, axis=0))`: normalize the embedding vector;\n",
        "\n",
        "`map(('id', 'vec'), 'insert_status', ops.ann_insert.milvus_client(host='127.0.0.1', port='19530', collection_name='question_answer'))`: insert question embedding vector into Milvus;"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b35657d0",
      "metadata": {
        "id": "b35657d0"
      },
      "source": [
        "### Ask Question with Milvus and Towhee"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd02adfc",
      "metadata": {
        "id": "cd02adfc"
      },
      "source": [
        "Now that embedding for question dataset have been inserted into Milvus, we can ask question with Milvus and Towhee. Again, we use Towhee to load the input question, compute a embedding, and use it as a query in Milvus. Because Milvus only outputs IDs and distance values, we provide the `id_answers` dictionary to get the answers based on IDs and display."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "95913f05",
      "metadata": {
        "id": "95913f05",
        "outputId": "ae5ea48a-7bf2-4854-edb0-1ef018e77303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 125 ms, sys: 641 ms, total: 766 ms\n",
            "Wall time: 23.9 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "collection.load()\n",
        "ans_pipe = (\n",
        "    pipe.input('question')\n",
        "        .map('question', 'vec', ops.text_embedding.dpr(model_name=\"facebook/dpr-ctx_encoder-single-nq-base\"))\n",
        "        .map('vec', 'vec', lambda x: x / np.linalg.norm(x, axis=0))\n",
        "        .map('vec', 'res', ops.ann_search.milvus_client(host=REMOTE_SERVER, port='19530', collection_name='question_answer', limit=1))\n",
        "        .map('res', 'answer', lambda x: [id_answer[int(i[0])] for i in x])\n",
        "        .output('question', 'answer')\n",
        ")\n",
        "ans = ans_pipe('Is  Disability  Insurance  Required  By  Law?')\n",
        "ans = DataCollection(ans)\n",
        "clear_output()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "98d36b22",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table style=\"border-collapse: collapse;\"><tr><th style=\"text-align: center; font-size: 130%; border: none;\">question</th> <th style=\"text-align: center; font-size: 130%; border: none;\">answer</th></tr>\n",
              "<tr><td style=\"text-align: center; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \">Is  Disability  Insurance  Required  By  Law?</td><td style=\"text-align: center; vertical-align: top; border-right: solid 1px #D3D3D3; border-left: solid 1px #D3D3D3; \"><br>Not generally. There are five states that require most all employers carry short term disability insurance on their employees. T...</br></td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ans.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfb05a79",
      "metadata": {
        "id": "bfb05a79"
      },
      "source": [
        "Then we can get the answer about 'Is  Disability  Insurance  Required  By  Law?'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "cb1a8f96",
      "metadata": {
        "id": "cb1a8f96",
        "outputId": "c19e3d2f-0e0b-47c5-ade8-85543543bdf8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Not generally. There are five states that require most all employers carry short term disability insurance on their employees. These states are: California, Hawaii, New Jersey, New York, and Rhode Island. Besides this mandatory short term disability law, there is no other legislative imperative for someone to purchase or be covered by disability insurance.']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ans[0]['answer']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01bef722",
      "metadata": {
        "id": "01bef722"
      },
      "source": [
        "## Release a Showcase"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c71cace8",
      "metadata": {
        "id": "c71cace8"
      },
      "source": [
        "We've done an excellent job on the core functionality of our question answering engine. Now it's time to build a showcase with interface. [Gradio](https://gradio.app/) is a great tool for building demos. With Gradio, we simply need to wrap the data processing pipeline via a `chat` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "65d42114",
      "metadata": {
        "id": "65d42114"
      },
      "outputs": [],
      "source": [
        "import towhee\n",
        "def chat(message, history):\n",
        "    history = history or []\n",
        "    ans_pipe = (\n",
        "        pipe.input('question')\n",
        "            .map('question', 'vec', ops.text_embedding.dpr(model_name=\"facebook/dpr-ctx_encoder-single-nq-base\"))\n",
        "            .map('vec', 'vec', lambda x: x / np.linalg.norm(x, axis=0))\n",
        "            .map('vec', 'res', ops.ann_search.milvus_client(host=REMOTE_SERVER, port='19530', collection_name='question_answer', limit=1))\n",
        "            .map('res', 'answer', lambda x: [id_answer[int(i[0])] for i in x])\n",
        "            .output('question', 'answer')\n",
        "    )\n",
        "    response = ans_pipe(message).get()[1][0]\n",
        "    history.append((message, response))\n",
        "    return history, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "67ed9a25",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-15 17:49:34,865 - 140590948419136 - _trace.py-_trace:45 - DEBUG: connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=3 socket_options=None\n",
            "2024-02-15 17:49:34,870 - 140590375175744 - _trace.py-_trace:45 - DEBUG: connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=5 socket_options=None\n",
            "2024-02-15 17:49:34,875 - 140590336312896 - selector_events.py-selector_events:54 - DEBUG: Using selector: EpollSelector\n",
            "2024-02-15 17:49:34,878 - 140601223811520 - _config.py-_config:78 - DEBUG: load_ssl_context verify=True cert=None trust_env=True http2=False\n",
            "2024-02-15 17:49:34,880 - 140601223811520 - _config.py-_config:144 - DEBUG: load_verify_locations cafile='/mnt/c/Blog/Watsonx-Assistant-with-Milvus-as-Vector-Database/.venv/lib/python3.10/site-packages/certifi/cacert.pem'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-15 17:49:34,937 - 140601223811520 - _trace.py-_trace:45 - DEBUG: connect_tcp.started host='127.0.0.1' port=7860 local_address=None timeout=5.0 socket_options=None\n",
            "2024-02-15 17:49:34,938 - 140601223811520 - _trace.py-_trace:45 - DEBUG: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fddbd8d9a50>\n",
            "2024-02-15 17:49:34,939 - 140601223811520 - _trace.py-_trace:45 - DEBUG: send_request_headers.started request=<Request [b'GET']>\n",
            "2024-02-15 17:49:34,940 - 140601223811520 - _trace.py-_trace:45 - DEBUG: send_request_headers.complete\n",
            "2024-02-15 17:49:34,942 - 140601223811520 - _trace.py-_trace:45 - DEBUG: send_request_body.started request=<Request [b'GET']>\n",
            "2024-02-15 17:49:34,942 - 140601223811520 - _trace.py-_trace:45 - DEBUG: send_request_body.complete\n",
            "2024-02-15 17:49:34,943 - 140601223811520 - _trace.py-_trace:45 - DEBUG: receive_response_headers.started request=<Request [b'GET']>\n",
            "2024-02-15 17:49:34,944 - 140601223811520 - _trace.py-_trace:45 - DEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 15 Feb 2024 16:49:34 GMT'), (b'server', b'uvicorn'), (b'content-length', b'5'), (b'content-type', b'application/json')])\n",
            "2024-02-15 17:49:34,945 - 140601223811520 - _client.py-_client:1027 - INFO: HTTP Request: GET http://127.0.0.1:7860/startup-events \"HTTP/1.1 200 OK\"\n",
            "2024-02-15 17:49:34,946 - 140601223811520 - _trace.py-_trace:45 - DEBUG: receive_response_body.started request=<Request [b'GET']>\n",
            "2024-02-15 17:49:34,946 - 140601223811520 - _trace.py-_trace:45 - DEBUG: receive_response_body.complete\n",
            "2024-02-15 17:49:34,947 - 140601223811520 - _trace.py-_trace:45 - DEBUG: response_closed.started\n",
            "2024-02-15 17:49:34,948 - 140601223811520 - _trace.py-_trace:45 - DEBUG: response_closed.complete\n",
            "2024-02-15 17:49:34,948 - 140601223811520 - _trace.py-_trace:45 - DEBUG: close.started\n",
            "2024-02-15 17:49:34,950 - 140601223811520 - _trace.py-_trace:45 - DEBUG: close.complete\n",
            "2024-02-15 17:49:34,952 - 140601223811520 - _config.py-_config:78 - DEBUG: load_ssl_context verify=False cert=None trust_env=True http2=False\n",
            "2024-02-15 17:49:34,953 - 140601223811520 - _trace.py-_trace:45 - DEBUG: connect_tcp.started host='127.0.0.1' port=7860 local_address=None timeout=3 socket_options=None\n",
            "2024-02-15 17:49:34,954 - 140601223811520 - _trace.py-_trace:45 - DEBUG: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fddbd8da1d0>\n",
            "2024-02-15 17:49:34,956 - 140601223811520 - _trace.py-_trace:45 - DEBUG: send_request_headers.started request=<Request [b'HEAD']>\n",
            "2024-02-15 17:49:34,958 - 140601223811520 - _trace.py-_trace:45 - DEBUG: send_request_headers.complete\n",
            "2024-02-15 17:49:34,959 - 140601223811520 - _trace.py-_trace:45 - DEBUG: send_request_body.started request=<Request [b'HEAD']>\n",
            "2024-02-15 17:49:34,960 - 140601223811520 - _trace.py-_trace:45 - DEBUG: send_request_body.complete\n",
            "2024-02-15 17:49:34,962 - 140601223811520 - _trace.py-_trace:45 - DEBUG: receive_response_headers.started request=<Request [b'HEAD']>\n",
            "2024-02-15 17:49:34,964 - 140601223811520 - _trace.py-_trace:45 - DEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'date', b'Thu, 15 Feb 2024 16:49:34 GMT'), (b'server', b'uvicorn'), (b'content-length', b'8851'), (b'content-type', b'text/html; charset=utf-8')])\n",
            "2024-02-15 17:49:34,965 - 140601223811520 - _client.py-_client:1027 - INFO: HTTP Request: HEAD http://127.0.0.1:7860/ \"HTTP/1.1 200 OK\"\n",
            "2024-02-15 17:49:34,966 - 140601223811520 - _trace.py-_trace:45 - DEBUG: receive_response_body.started request=<Request [b'HEAD']>\n",
            "2024-02-15 17:49:34,967 - 140601223811520 - _trace.py-_trace:45 - DEBUG: receive_response_body.complete\n",
            "2024-02-15 17:49:34,967 - 140601223811520 - _trace.py-_trace:45 - DEBUG: response_closed.started\n",
            "2024-02-15 17:49:34,968 - 140601223811520 - _trace.py-_trace:45 - DEBUG: response_closed.complete\n",
            "2024-02-15 17:49:34,969 - 140601223811520 - _trace.py-_trace:45 - DEBUG: close.started\n",
            "2024-02-15 17:49:34,970 - 140601223811520 - _trace.py-_trace:45 - DEBUG: close.complete\n",
            "2024-02-15 17:49:34,972 - 140601223811520 - _config.py-_config:78 - DEBUG: load_ssl_context verify=True cert=None trust_env=True http2=False\n",
            "2024-02-15 17:49:34,973 - 140601223811520 - _config.py-_config:144 - DEBUG: load_verify_locations cafile='/mnt/c/Blog/Watsonx-Assistant-with-Milvus-as-Vector-Database/.venv/lib/python3.10/site-packages/certifi/cacert.pem'\n",
            "2024-02-15 17:49:35,021 - 140601223811520 - _trace.py-_trace:45 - DEBUG: connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=30 socket_options=None\n",
            "2024-02-15 17:49:35,072 - 140590948419136 - _trace.py-_trace:45 - DEBUG: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fddbd8ab4c0>\n",
            "2024-02-15 17:49:35,072 - 140590375175744 - _trace.py-_trace:45 - DEBUG: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fddbd89b3d0>\n",
            "2024-02-15 17:49:35,073 - 140590948419136 - _trace.py-_trace:45 - DEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x7fddbf67da40> server_hostname='api.gradio.app' timeout=3\n",
            "2024-02-15 17:49:35,074 - 140590375175744 - _trace.py-_trace:45 - DEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x7fddbf67d1c0> server_hostname='api.gradio.app' timeout=5\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-15 17:49:35,223 - 140601223811520 - _trace.py-_trace:45 - DEBUG: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fddbd8da050>\n",
            "2024-02-15 17:49:35,224 - 140601223811520 - _trace.py-_trace:45 - DEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x7fddbf67e340> server_hostname='api.gradio.app' timeout=30\n",
            "2024-02-15 17:49:35,464 - 140590948419136 - _trace.py-_trace:45 - DEBUG: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fddbd8ab490>\n",
            "2024-02-15 17:49:35,465 - 140590948419136 - _trace.py-_trace:45 - DEBUG: send_request_headers.started request=<Request [b'GET']>\n",
            "2024-02-15 17:49:35,467 - 140590948419136 - _trace.py-_trace:45 - DEBUG: send_request_headers.complete\n",
            "2024-02-15 17:49:35,467 - 140590948419136 - _trace.py-_trace:45 - DEBUG: send_request_body.started request=<Request [b'GET']>\n",
            "2024-02-15 17:49:35,468 - 140590948419136 - _trace.py-_trace:45 - DEBUG: send_request_body.complete\n",
            "2024-02-15 17:49:35,468 - 140590948419136 - _trace.py-_trace:45 - DEBUG: receive_response_headers.started request=<Request [b'GET']>\n",
            "2024-02-15 17:49:35,469 - 140590375175744 - _trace.py-_trace:45 - DEBUG: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fddbd89b3a0>\n",
            "2024-02-15 17:49:35,470 - 140590375175744 - _trace.py-_trace:45 - DEBUG: send_request_headers.started request=<Request [b'POST']>\n",
            "2024-02-15 17:49:35,471 - 140590375175744 - _trace.py-_trace:45 - DEBUG: send_request_headers.complete\n",
            "2024-02-15 17:49:35,472 - 140590375175744 - _trace.py-_trace:45 - DEBUG: send_request_body.started request=<Request [b'POST']>\n",
            "2024-02-15 17:49:35,472 - 140590375175744 - _trace.py-_trace:45 - DEBUG: send_request_body.complete\n",
            "2024-02-15 17:49:35,473 - 140590375175744 - _trace.py-_trace:45 - DEBUG: receive_response_headers.started request=<Request [b'POST']>\n",
            "2024-02-15 17:49:35,617 - 140601223811520 - _trace.py-_trace:45 - DEBUG: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fddbd8da980>\n",
            "2024-02-15 17:49:35,618 - 140601223811520 - _trace.py-_trace:45 - DEBUG: send_request_headers.started request=<Request [b'GET']>\n",
            "2024-02-15 17:49:35,619 - 140601223811520 - _trace.py-_trace:45 - DEBUG: send_request_headers.complete\n",
            "2024-02-15 17:49:35,619 - 140601223811520 - _trace.py-_trace:45 - DEBUG: send_request_body.started request=<Request [b'GET']>\n",
            "2024-02-15 17:49:35,620 - 140601223811520 - _trace.py-_trace:45 - DEBUG: send_request_body.complete\n",
            "2024-02-15 17:49:35,620 - 140601223811520 - _trace.py-_trace:45 - DEBUG: receive_response_headers.started request=<Request [b'GET']>\n",
            "2024-02-15 17:49:35,663 - 140590948419136 - _trace.py-_trace:45 - DEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 15 Feb 2024 16:49:35 GMT'), (b'Content-Type', b'application/json'), (b'Content-Length', b'21'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*')])\n",
            "2024-02-15 17:49:35,666 - 140590948419136 - _client.py-_client:1027 - INFO: HTTP Request: GET https://api.gradio.app/pkg-version \"HTTP/1.1 200 OK\"\n",
            "2024-02-15 17:49:35,667 - 140590948419136 - _trace.py-_trace:45 - DEBUG: receive_response_body.started request=<Request [b'GET']>\n",
            "2024-02-15 17:49:35,667 - 140590948419136 - _trace.py-_trace:45 - DEBUG: receive_response_body.complete\n",
            "2024-02-15 17:49:35,668 - 140590948419136 - _trace.py-_trace:45 - DEBUG: response_closed.started\n",
            "2024-02-15 17:49:35,669 - 140590948419136 - _trace.py-_trace:45 - DEBUG: response_closed.complete\n",
            "2024-02-15 17:49:35,670 - 140590948419136 - _trace.py-_trace:45 - DEBUG: close.started\n",
            "2024-02-15 17:49:35,670 - 140590948419136 - _trace.py-_trace:45 - DEBUG: close.complete\n",
            "2024-02-15 17:49:35,779 - 140590375175744 - _trace.py-_trace:45 - DEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 15 Feb 2024 16:49:36 GMT'), (b'Content-Type', b'text/html; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*'), (b'Content-Encoding', b'gzip')])\n",
            "2024-02-15 17:49:35,780 - 140590375175744 - _client.py-_client:1027 - INFO: HTTP Request: POST https://api.gradio.app/gradio-initiated-analytics/ \"HTTP/1.1 200 OK\"\n",
            "2024-02-15 17:49:35,781 - 140590375175744 - _trace.py-_trace:45 - DEBUG: receive_response_body.started request=<Request [b'POST']>\n",
            "2024-02-15 17:49:35,783 - 140590375175744 - _trace.py-_trace:45 - DEBUG: receive_response_body.complete\n",
            "2024-02-15 17:49:35,783 - 140590375175744 - _trace.py-_trace:45 - DEBUG: response_closed.started\n",
            "2024-02-15 17:49:35,784 - 140590375175744 - _trace.py-_trace:45 - DEBUG: response_closed.complete\n",
            "2024-02-15 17:49:35,785 - 140590375175744 - _trace.py-_trace:45 - DEBUG: close.started\n",
            "2024-02-15 17:49:35,786 - 140590375175744 - _trace.py-_trace:45 - DEBUG: close.complete\n",
            "2024-02-15 17:49:44,732 - 140601223811520 - _trace.py-_trace:45 - DEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 15 Feb 2024 16:49:44 GMT'), (b'Content-Type', b'text/html; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'ContentType', b'application/json'), (b'Access-Control-Allow-Origin', b'*'), (b'Content-Encoding', b'gzip')])\n",
            "2024-02-15 17:49:44,733 - 140601223811520 - _client.py-_client:1027 - INFO: HTTP Request: GET https://api.gradio.app/v2/tunnel-request \"HTTP/1.1 200 OK\"\n",
            "2024-02-15 17:49:44,733 - 140601223811520 - _trace.py-_trace:45 - DEBUG: receive_response_body.started request=<Request [b'GET']>\n",
            "2024-02-15 17:49:44,734 - 140601223811520 - _trace.py-_trace:45 - DEBUG: receive_response_body.complete\n",
            "2024-02-15 17:49:44,735 - 140601223811520 - _trace.py-_trace:45 - DEBUG: response_closed.started\n",
            "2024-02-15 17:49:44,735 - 140601223811520 - _trace.py-_trace:45 - DEBUG: response_closed.complete\n",
            "2024-02-15 17:49:44,736 - 140601223811520 - _trace.py-_trace:45 - DEBUG: close.started\n",
            "2024-02-15 17:49:44,737 - 140601223811520 - _trace.py-_trace:45 - DEBUG: close.complete\n",
            "2024-02-15 17:49:44,738 - 140601223811520 - _config.py-_config:78 - DEBUG: load_ssl_context verify=True cert=None trust_env=True http2=False\n",
            "2024-02-15 17:49:44,739 - 140601223811520 - _config.py-_config:144 - DEBUG: load_verify_locations cafile='/mnt/c/Blog/Watsonx-Assistant-with-Milvus-as-Vector-Database/.venv/lib/python3.10/site-packages/certifi/cacert.pem'\n",
            "2024-02-15 17:49:44,782 - 140601223811520 - _trace.py-_trace:45 - DEBUG: connect_tcp.started host='cdn-media.huggingface.co' port=443 local_address=None timeout=30 socket_options=None\n",
            "2024-02-15 17:49:44,803 - 140601223811520 - _trace.py-_trace:45 - DEBUG: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fddbd8db520>\n",
            "2024-02-15 17:49:44,804 - 140601223811520 - _trace.py-_trace:45 - DEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x7fddbf67e3c0> server_hostname='cdn-media.huggingface.co' timeout=30\n",
            "2024-02-15 17:49:44,819 - 140601223811520 - _trace.py-_trace:45 - DEBUG: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fddbd8db5e0>\n",
            "2024-02-15 17:49:44,820 - 140601223811520 - _trace.py-_trace:45 - DEBUG: send_request_headers.started request=<Request [b'GET']>\n",
            "2024-02-15 17:49:44,821 - 140601223811520 - _trace.py-_trace:45 - DEBUG: send_request_headers.complete\n",
            "2024-02-15 17:49:44,822 - 140601223811520 - _trace.py-_trace:45 - DEBUG: send_request_body.started request=<Request [b'GET']>\n",
            "2024-02-15 17:49:44,823 - 140601223811520 - _trace.py-_trace:45 - DEBUG: send_request_body.complete\n",
            "2024-02-15 17:49:44,823 - 140601223811520 - _trace.py-_trace:45 - DEBUG: receive_response_headers.started request=<Request [b'GET']>\n",
            "2024-02-15 17:49:44,835 - 140601223811520 - _trace.py-_trace:45 - DEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Type', b'binary/octet-stream'), (b'Content-Length', b'11374592'), (b'Connection', b'keep-alive'), (b'Last-Modified', b'Tue, 04 Jul 2023 12:26:37 GMT'), (b'x-amz-server-side-encryption', b'AES256'), (b'Accept-Ranges', b'bytes'), (b'Server', b'AmazonS3'), (b'Date', b'Thu, 15 Feb 2024 10:36:26 GMT'), (b'ETag', b'\"3a6ef613fc0d9adbca1393479748524d-2\"'), (b'Vary', b'Accept-Encoding'), (b'X-Cache', b'Hit from cloudfront'), (b'Via', b'1.1 5d27236a5f6fb9836424f23bb92b0cd0.cloudfront.net (CloudFront)'), (b'X-Amz-Cf-Pop', b'MXP64-P1'), (b'X-Amz-Cf-Id', b'b5m5xVQ16PxGzuFuHixNVmYUY0hxSyRcjtM6Anu563jfekIVoqjFYg=='), (b'Age', b'22423')])\n",
            "2024-02-15 17:49:44,836 - 140601223811520 - _client.py-_client:1027 - INFO: HTTP Request: GET https://cdn-media.huggingface.co/frpc-gradio-0.2/frpc_linux_amd64 \"HTTP/1.1 200 OK\"\n",
            "2024-02-15 17:49:44,837 - 140601223811520 - _trace.py-_trace:45 - DEBUG: receive_response_body.started request=<Request [b'GET']>\n",
            "2024-02-15 17:49:45,120 - 140601223811520 - _trace.py-_trace:45 - DEBUG: receive_response_body.complete\n",
            "2024-02-15 17:49:45,121 - 140601223811520 - _trace.py-_trace:45 - DEBUG: response_closed.started\n",
            "2024-02-15 17:49:45,122 - 140601223811520 - _trace.py-_trace:45 - DEBUG: response_closed.complete\n",
            "2024-02-15 17:49:45,125 - 140601223811520 - _trace.py-_trace:45 - DEBUG: close.started\n",
            "2024-02-15 17:49:45,126 - 140601223811520 - _trace.py-_trace:45 - DEBUG: close.complete\n",
            "2024-02-15 17:49:45,299 - 140590375175744 - _config.py-_config:78 - DEBUG: load_ssl_context verify=True cert=None trust_env=True http2=False\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-15 17:49:45,303 - 140590375175744 - _config.py-_config:144 - DEBUG: load_verify_locations cafile='/mnt/c/Blog/Watsonx-Assistant-with-Milvus-as-Vector-Database/.venv/lib/python3.10/site-packages/certifi/cacert.pem'\n",
            "2024-02-15 17:49:45,305 - 140590948419136 - _config.py-_config:78 - DEBUG: load_ssl_context verify=True cert=None trust_env=True http2=False\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-15 17:49:45,308 - 140590948419136 - _config.py-_config:144 - DEBUG: load_verify_locations cafile='/mnt/c/Blog/Watsonx-Assistant-with-Milvus-as-Vector-Database/.venv/lib/python3.10/site-packages/certifi/cacert.pem'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-02-15 17:49:45,391 - 140590375175744 - _trace.py-_trace:45 - DEBUG: connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=5 socket_options=None\n",
            "2024-02-15 17:49:45,395 - 140590948419136 - _trace.py-_trace:45 - DEBUG: connect_tcp.started host='api.gradio.app' port=443 local_address=None timeout=5 socket_options=None\n",
            "2024-02-15 17:49:45,596 - 140590375175744 - _trace.py-_trace:45 - DEBUG: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fddbd37b4c0>\n",
            "2024-02-15 17:49:45,598 - 140590375175744 - _trace.py-_trace:45 - DEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x7fddbf67e440> server_hostname='api.gradio.app' timeout=5\n",
            "2024-02-15 17:49:45,612 - 140590948419136 - _trace.py-_trace:45 - DEBUG: connect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fddbd37b550>\n",
            "2024-02-15 17:49:45,613 - 140590948419136 - _trace.py-_trace:45 - DEBUG: start_tls.started ssl_context=<ssl.SSLContext object at 0x7fddbf67e4c0> server_hostname='api.gradio.app' timeout=5\n",
            "2024-02-15 17:49:45,996 - 140590375175744 - _trace.py-_trace:45 - DEBUG: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fddbd37b4f0>\n",
            "2024-02-15 17:49:45,997 - 140590375175744 - _trace.py-_trace:45 - DEBUG: send_request_headers.started request=<Request [b'POST']>\n",
            "2024-02-15 17:49:45,999 - 140590375175744 - _trace.py-_trace:45 - DEBUG: send_request_headers.complete\n",
            "2024-02-15 17:49:46,000 - 140590375175744 - _trace.py-_trace:45 - DEBUG: send_request_body.started request=<Request [b'POST']>\n",
            "2024-02-15 17:49:46,001 - 140590375175744 - _trace.py-_trace:45 - DEBUG: send_request_body.complete\n",
            "2024-02-15 17:49:46,002 - 140590375175744 - _trace.py-_trace:45 - DEBUG: receive_response_headers.started request=<Request [b'POST']>\n",
            "2024-02-15 17:49:46,015 - 140590948419136 - _trace.py-_trace:45 - DEBUG: start_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x7fddbd8dae00>\n",
            "2024-02-15 17:49:46,016 - 140590948419136 - _trace.py-_trace:45 - DEBUG: send_request_headers.started request=<Request [b'POST']>\n",
            "2024-02-15 17:49:46,018 - 140590948419136 - _trace.py-_trace:45 - DEBUG: send_request_headers.complete\n",
            "2024-02-15 17:49:46,019 - 140590948419136 - _trace.py-_trace:45 - DEBUG: send_request_body.started request=<Request [b'POST']>\n",
            "2024-02-15 17:49:46,021 - 140590948419136 - _trace.py-_trace:45 - DEBUG: send_request_body.complete\n",
            "2024-02-15 17:49:46,021 - 140590948419136 - _trace.py-_trace:45 - DEBUG: receive_response_headers.started request=<Request [b'POST']>\n",
            "2024-02-15 17:49:46,322 - 140590375175744 - _trace.py-_trace:45 - DEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 15 Feb 2024 16:49:46 GMT'), (b'Content-Type', b'text/html; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*'), (b'Content-Encoding', b'gzip')])\n",
            "2024-02-15 17:49:46,323 - 140590375175744 - _client.py-_client:1027 - INFO: HTTP Request: POST https://api.gradio.app/gradio-error-analytics/ \"HTTP/1.1 200 OK\"\n",
            "2024-02-15 17:49:46,324 - 140590375175744 - _trace.py-_trace:45 - DEBUG: receive_response_body.started request=<Request [b'POST']>\n",
            "2024-02-15 17:49:46,325 - 140590375175744 - _trace.py-_trace:45 - DEBUG: receive_response_body.complete\n",
            "2024-02-15 17:49:46,326 - 140590375175744 - _trace.py-_trace:45 - DEBUG: response_closed.started\n",
            "2024-02-15 17:49:46,326 - 140590375175744 - _trace.py-_trace:45 - DEBUG: response_closed.complete\n",
            "2024-02-15 17:49:46,327 - 140590375175744 - _trace.py-_trace:45 - DEBUG: close.started\n",
            "2024-02-15 17:49:46,328 - 140590375175744 - _trace.py-_trace:45 - DEBUG: close.complete\n",
            "2024-02-15 17:49:46,335 - 140590948419136 - _trace.py-_trace:45 - DEBUG: receive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Date', b'Thu, 15 Feb 2024 16:49:46 GMT'), (b'Content-Type', b'text/html; charset=utf-8'), (b'Transfer-Encoding', b'chunked'), (b'Connection', b'keep-alive'), (b'Server', b'nginx/1.18.0'), (b'Access-Control-Allow-Origin', b'*'), (b'Content-Encoding', b'gzip')])\n",
            "2024-02-15 17:49:46,336 - 140590948419136 - _client.py-_client:1027 - INFO: HTTP Request: POST https://api.gradio.app/gradio-launched-telemetry/ \"HTTP/1.1 200 OK\"\n",
            "2024-02-15 17:49:46,337 - 140590948419136 - _trace.py-_trace:45 - DEBUG: receive_response_body.started request=<Request [b'POST']>\n",
            "2024-02-15 17:49:46,338 - 140590948419136 - _trace.py-_trace:45 - DEBUG: receive_response_body.complete\n",
            "2024-02-15 17:49:46,340 - 140590948419136 - _trace.py-_trace:45 - DEBUG: response_closed.started\n",
            "2024-02-15 17:49:46,341 - 140590948419136 - _trace.py-_trace:45 - DEBUG: response_closed.complete\n",
            "2024-02-15 17:49:46,341 - 140590948419136 - _trace.py-_trace:45 - DEBUG: close.started\n",
            "2024-02-15 17:49:46,342 - 140590948419136 - _trace.py-_trace:45 - DEBUG: close.complete\n",
            "2024-02-15 17:50:47,703 - 140590234535488 - connectionpool.py-connectionpool:1055 - DEBUG: Starting new HTTPS connection (1): huggingface.co:443\n",
            "2024-02-15 17:50:47,946 - 140590234535488 - connectionpool.py-connectionpool:549 - DEBUG: https://huggingface.co:443 \"HEAD /facebook/dpr-ctx_encoder-single-nq-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
            "2024-02-15 17:50:48,188 - 140590234535488 - connectionpool.py-connectionpool:549 - DEBUG: https://huggingface.co:443 \"HEAD /facebook/dpr-ctx_encoder-single-nq-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
            "2024-02-15 17:50:48,329 - 140590234535488 - connectionpool.py-connectionpool:549 - DEBUG: https://huggingface.co:443 \"HEAD /facebook/dpr-ctx_encoder-single-nq-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
            "2024-02-15 17:50:51,438 - 140589169247808 - node.py-node:167 - INFO: Begin to run Node-_input\n",
            "2024-02-15 17:50:51,439 - 140589177701952 - node.py-node:167 - INFO: Begin to run Node-text-embedding/dpr-0\n",
            "2024-02-15 17:50:51,440 - 140589160793664 - node.py-node:167 - INFO: Begin to run Node-lambda-1\n",
            "2024-02-15 17:50:51,441 - 140589152339520 - node.py-node:167 - INFO: Begin to run Node-ann-search/milvus-client-2\n",
            "2024-02-15 17:50:51,442 - 140589169247808 - node.py-node:167 - INFO: Begin to run Node-lambda-3\n",
            "2024-02-15 17:50:51,442 - 140589143885376 - node.py-node:167 - INFO: Begin to run Node-_output\n",
            "2024-02-15 17:52:58,912 - 140589169247808 - connectionpool.py-connectionpool:1055 - DEBUG: Starting new HTTPS connection (1): huggingface.co:443\n",
            "2024-02-15 17:52:59,147 - 140589169247808 - connectionpool.py-connectionpool:549 - DEBUG: https://huggingface.co:443 \"HEAD /facebook/dpr-ctx_encoder-single-nq-base/resolve/main/tokenizer_config.json HTTP/1.1\" 200 0\n",
            "2024-02-15 17:52:59,347 - 140589169247808 - connectionpool.py-connectionpool:549 - DEBUG: https://huggingface.co:443 \"HEAD /facebook/dpr-ctx_encoder-single-nq-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
            "2024-02-15 17:52:59,479 - 140589169247808 - connectionpool.py-connectionpool:549 - DEBUG: https://huggingface.co:443 \"HEAD /facebook/dpr-ctx_encoder-single-nq-base/resolve/main/config.json HTTP/1.1\" 200 0\n",
            "2024-02-15 17:53:01,845 - 140589058557504 - node.py-node:167 - INFO: Begin to run Node-_input\n",
            "2024-02-15 17:53:01,845 - 140589067011648 - node.py-node:167 - INFO: Begin to run Node-text-embedding/dpr-0\n",
            "2024-02-15 17:53:01,846 - 140589058557504 - node.py-node:167 - INFO: Begin to run Node-lambda-1\n",
            "2024-02-15 17:53:01,847 - 140588707939904 - node.py-node:167 - INFO: Begin to run Node-ann-search/milvus-client-2\n",
            "2024-02-15 17:53:01,847 - 140588699485760 - node.py-node:167 - INFO: Begin to run Node-lambda-3\n",
            "2024-02-15 17:53:01,848 - 140588691031616 - node.py-node:167 - INFO: Begin to run Node-_output\n"
          ]
        }
      ],
      "source": [
        "import gradio\n",
        "\n",
        "collection.load()\n",
        "chatbot = gradio.Chatbot()\n",
        "interface = gradio.Interface(\n",
        "    chat,\n",
        "    [\"text\", \"state\"],\n",
        "    [chatbot, \"state\"],\n",
        "    allow_flagging=\"never\",\n",
        ")\n",
        "clear_output()\n",
        "interface.launch(inline=True, share=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "f7dd10cdbe9a9c71f7e71741efd428241b5f9fa0fecdd29ae07a5706cd5ff8a2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
